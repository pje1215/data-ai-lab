import streamlit as st
import pandas as pd
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
import datetime

# -----------------------------
# 0. ê¸°ë³¸ ì„¤ì • (ì¤‘ë¦½í™”)
# -----------------------------
st.set_page_config(page_title="ìƒí’ˆëª… ìë™ ë¶„ë¥˜ê¸°", layout="wide")
st.title("ğŸ§© ìƒí’ˆëª… ìë™ ë¶„ë¥˜ ì‹œìŠ¤í…œ")
st.caption("ì—°êµ¬/PoC ë°ëª¨ | front/back embedding ê¸°ë°˜ ì˜ë¯¸ê¸°ë°˜ ìë™êµ°ì§‘")

# -----------------------------
# 1. ì—‘ì…€ ì—…ë¡œë“œ
# -----------------------------
uploaded_file = st.file_uploader("ìƒí’ˆëª… ë°ì´í„° ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])
if uploaded_file is not None:
    data = pd.read_excel(uploaded_file)
    st.success(f"{data.shape[0]}ê°œì˜ í–‰ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    st.stop()

# -----------------------------
# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ê·¸ëŒ€ë¡œ ìœ ì§€, ë„ë©”ì¸ í‘œí˜„ ì¤‘ë¦½í™”)
# -----------------------------
def clean_text(text):
    text = str(text)
    text = re.sub(r"[\[\]\(\)]", " ", text)
    text = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip().lower()

# íŠ¹ìˆ˜ ê³µë°± ì •ê·œí™”
_WS_HARD = r"[\u00A0\u202F\u2007\u2000-\u2006\u2008-\u200A\u200B-\u200D\u2060]"
def _normalize_spaces(s: str) -> str:
    s = re.sub(_WS_HARD, " ", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

# í¼ì§€ íŒ¨í„´ : íŠ¹ìˆ˜ë¬¸ìÂ·ë„ì–´ì“°ê¸° ë³€í™”ì— ìƒê´€ì—†ì´ ìœ ì—°í•˜ê²Œ ë§¤ì¹­í•˜ê¸° ìœ„í•œ ì •ê·œì‹ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ê¸°
def _build_fuzzy_pattern(cat: str) -> str:
    base = re.sub(_WS_HARD, "", cat)
    base = re.sub(r"[\s\W_]+", "", base)
    parts = [re.escape(ch) for ch in base]
    if not parts:
        return r"$^"
    return r"(?:[\s\W_]*)".join(parts)

# L_category/ì‚¬ì´íŠ¸ì´ë¦„ ì œê±°í•˜ì—¬ í•µì‹¬ í† í°ë§Œ ë‚¨ê¸°ê¸°
def clean_and_trim_text(row):
    text = str(row["pname"])
    category = str(row["L_category"])
    site_name = str(row.get("ì‚¬ì´íŠ¸ì´ë¦„", ""))
    text = re.sub(r"[\[\]\(\)]", " ", text)
    text_norm = _normalize_spaces(text)
    if category and category != "nan":
        cat_pat = _build_fuzzy_pattern(_normalize_spaces(category))
        text_norm = re.sub(cat_pat, " ", text_norm, flags=re.IGNORECASE)
    if site_name and site_name != "nan":
        site_pat = _build_fuzzy_pattern(_normalize_spaces(site_name))
        text_norm = re.sub(site_pat, " ", text_norm, flags=re.IGNORECASE)
    text_norm = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", text_norm)
    return _normalize_spaces(text_norm).lower()

# -----------------------------
# 3. ì „ì²˜ë¦¬ ì‹¤í–‰
# -----------------------------
data["ìƒí’ˆëª…_clean"] = data["pname"].astype(str).apply(clean_text)
data["ìƒí’ˆëª…_trim"] = data.apply(clean_and_trim_text, axis=1)

site = st.selectbox("ì‚¬ì´íŠ¸ ì„ íƒ", sorted(data["site_code"].unique()))
cats = st.multiselect("L_category ì„ íƒ", sorted(data["L_category"].unique()))
df_test = data[(data["site_code"] == site) & (data["L_category"].isin(cats))].copy()
st.write(f"ì„ íƒëœ ë°ì´í„°: {df_test.shape[0]}ê°œ")

# -----------------------------
# 4. 1ë‹¨ê³„: ì•ë‹¨ ì¤‘ì‹¬ ì„ë² ë”© + í´ëŸ¬ìŠ¤í„°ë§
# -----------------------------
@st.cache_resource(show_spinner=True)
def get_model():
    # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© (ì¤‘ë¦½ ëª¨ë¸ëª…)
    return SentenceTransformer("jhgan/ko-sroberta-multitask")

model = get_model()

def front_only_embedding(text, front_ratio=0.7, min_tokens=2, decay_rate=0.25):
    tokens = re.split(r"\s+", str(text).strip())
    n = len(tokens)
    if n == 0:
        return np.zeros(model.get_sentence_embedding_dimension())
    split_idx = max(min_tokens, int(n * front_ratio))
    front_tokens = tokens[:split_idx]
    back_tokens = tokens[split_idx:]
    # ì•ë‹¨ í† í° ê°€ì¤‘ í‰ê· 
    front_emb = model.encode(front_tokens, show_progress_bar=False)
    front_weights = np.array([np.exp(-decay_rate * i) for i in range(len(front_tokens))])
    front_weights /= front_weights.sum()
    front_vec = np.average(front_emb, axis=0, weights=front_weights)
    # ë’·ë‹¨ì€ ë¯¸ì„¸ ê°€ì¤‘ì¹˜
    back_vec = (np.mean(model.encode(back_tokens, show_progress_bar=False), axis=0) * 0.01
                if back_tokens else np.zeros(model.get_sentence_embedding_dimension()))
    return front_vec + back_vec

def extract_core_phrase(text):
    text = str(text)
    text = re.sub(r"\bv\d+|\bver\d+|\b\d{2,4}(\.\d+)?\b", "", text)
    text = re.sub(r"(êµì¬|í¬í•¨|ë¯¸í¬í•¨|only|ì˜¨ë¦¬|ver|ë²„ì „)", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = text.split()
    core = " ".join(tokens[:3]) if len(tokens) > 3 else text
    return core.strip()

if st.button("1ë‹¨ê³„ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"):
    cluster_results = []
    for cat in sorted(df_test["L_category"].unique()):
        subset = df_test[df_test["L_category"] == cat].copy()
        if len(subset) < 2:
            continue
        subset["embedding_front"] = subset["ìƒí’ˆëª…_trim"].apply(front_only_embedding)
        X_front = np.vstack(subset["embedding_front"].values)
        cluster_front = AgglomerativeClustering(
            n_clusters=None, distance_threshold=9, linkage='ward'
        )
        subset["cluster_lv1"] = cluster_front.fit_predict(X_front)
        cluster_results.append(subset)

    if cluster_results:
        df_lv1 = pd.concat(cluster_results, ignore_index=True)
        df_lv1["ëŒ€í‘œí•µì‹¬ì–´"] = df_lv1["ìƒí’ˆëª…_trim"].apply(extract_core_phrase)

        rep_lv1 = (
            df_lv1.groupby(["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "cluster_lv1"])
            .agg(ëŒ€í‘œí•µì‹¬ì–´=("ëŒ€í‘œí•µì‹¬ì–´", lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]))
            .reset_index()
        )
        rep_lv1["ëŒ€í‘œ_lv1ëª…"] = rep_lv1.apply(
            lambda row: f"{row['ì‚¬ì´íŠ¸ì´ë¦„']}_{row['L_category']}_{row['ëŒ€í‘œí•µì‹¬ì–´']}", axis=1
        )

        df_lv1 = df_lv1.merge(
            rep_lv1[["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "cluster_lv1", "ëŒ€í‘œ_lv1ëª…"]],
            on=["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "cluster_lv1"],
            how="left"
        )

        # âœ… 1ë‹¨ê³„ ê²°ê³¼ ì €ì¥ (2ë‹¨ê³„ ë° ë‹¤ìš´ë¡œë“œì—ì„œ í™œìš©)
        st.session_state["df_lv1"] = df_lv1

        # í‘œ
        st.dataframe(
            df_lv1[["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "ëŒ€í‘œ_lv1ëª…", "pname", "cluster_lv1"]]
            .sort_values(["L_category", "cluster_lv1"])
            .reset_index(drop=True)
        )

        # -----------------------------
        # ğŸ“Š L_category > ëŒ€í‘œ ê·¸ë£¹ë³„ ë¬¶ì¸ ìƒí’ˆ ì‹œê°í™”
        # -----------------------------
        st.markdown("### ğŸ“‚ L_categoryë³„ ëŒ€í‘œ ê·¸ë£¹ ì‹œê°í™”")
        for cat, cat_group in df_lv1.groupby("L_category"):
            st.markdown(f"#### ğŸ—‚ï¸ {cat}")
            for rep_name, subset in cat_group.groupby("ëŒ€í‘œ_lv1ëª…"):
                with st.expander(f"ğŸ“¦ {rep_name} ({len(subset)}ê°œ ìƒí’ˆ)"):
                    for s in subset["pname"].tolist():
                        st.write(f" - {s}")
            st.markdown("---")
    else:
        st.warning("ì„ íƒëœ ë°ì´í„°ì—ì„œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•  ì¶©ë¶„í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

# -----------------------------
# 5. 2ë‹¨ê³„: ë’·ë‹¨ ì¤‘ì‹¬ ì„¸ë¶„í™” (ì„ íƒ)
# -----------------------------
st.markdown("---")
st.subheader("2ë‹¨ê³„ ì„¸ë¶„í™” (ì„ íƒ)")

def back_only_embedding(text, back_ratio=0.5, min_tokens=2, decay_rate=0.25):
    tokens = re.split(r"\s+", str(text).strip())
    n = len(tokens)
    if n == 0:
        return np.zeros(model.get_sentence_embedding_dimension())
    split_idx = max(n - max(min_tokens, int(n * back_ratio)), 0)
    front_tokens = tokens[:split_idx]
    back_tokens = tokens[split_idx:]
    back_emb = model.encode(back_tokens, show_progress_bar=False)
    back_weights = np.array([np.exp(-decay_rate * i) for i in range(len(back_tokens))])[::-1]
    back_weights /= back_weights.sum()
    back_vec = np.average(back_emb, axis=0, weights=back_weights)
    front_vec = (np.mean(model.encode(front_tokens, show_progress_bar=False), axis=0) * 0.01
                 if front_tokens else np.zeros(model.get_sentence_embedding_dimension()))
    return back_vec + front_vec

if st.button("2ë‹¨ê³„ ì„¸ë¶„í™” ì‹¤í–‰"):
    if "df_lv1" not in st.session_state:
        st.warning(" ë¨¼ì € 1ë‹¨ê³„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.stop()

    df_lv1 = st.session_state["df_lv1"].copy()
    st.info("2ë‹¨ê³„ ì„¸ë¶„í™”ë¥¼ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")

    cluster_results = []
    for rep in sorted(df_lv1["cluster_lv1"].unique()):
        subset = df_lv1[df_lv1["cluster_lv1"] == rep].copy()
        if len(subset) < 3:
            subset["cluster_lv2"] = 0
            cluster_results.append(subset)
            continue
        subset["embedding_back"] = subset["ìƒí’ˆëª…_trim"].apply(back_only_embedding)
        X_back = np.vstack(subset["embedding_back"].values)
        cluster_back = AgglomerativeClustering(
            n_clusters=None, distance_threshold=5, linkage='ward'
        )
        subset["cluster_lv2"] = cluster_back.fit_predict(X_back)
        cluster_results.append(subset)

    df_lv2 = pd.concat(cluster_results, ignore_index=True) if cluster_results else df_lv1.assign(cluster_lv2=0)

    # âœ… 2ë‹¨ê³„ ê²°ê³¼ ì €ì¥ (ë‹¤ìš´ë¡œë“œ íƒ­ì—ì„œ ì‚¬ìš©)
    st.session_state["df_lv2"] = df_lv2

    # í‘œ
    st.dataframe(
        df_lv2[["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "ëŒ€í‘œ_lv1ëª…", "pname", "cluster_lv1", "cluster_lv2"]]
        .sort_values(["L_category", "cluster_lv1", "cluster_lv2"])
        .reset_index(drop=True)
    )

    # ğŸ“Š ì„¸ë¶„í™” ê²°ê³¼ ì‹œê°í™” (ì •ëˆëœ ê³„ì¸µ êµ¬ì¡°)
    st.markdown("### ğŸ“‚ L_category > 1ì°¨ ë¶„ë¥˜ > ì„¸ë¶„í™” ê²°ê³¼ ì‹œê°í™”")
    for cat, cat_group in df_lv2.groupby("L_category"):
        st.markdown(f"<h4>ğŸ“ {cat}</h4>", unsafe_allow_html=True)
        for lv1_name, lv1_group in cat_group.groupby("ëŒ€í‘œ_lv1ëª…"):
            # 1ì°¨ ë¶„ë¥˜ëª…
            st.markdown(
                f"<div style='margin-left:10px; color:#555; font-size:13px; font-style:italic;'>"
                f"1ì°¨ ë¶„ë¥˜ëª…: {lv1_name}</div>",
                unsafe_allow_html=True
            )
            # ì‹¤ì œ ìƒí’ˆëª…ë§Œ ë‚˜ì—´
            product_list = lv1_group["pname"].unique().tolist()
            for s in product_list:
                st.markdown(
                    f"<div style='margin-left:35px; font-size:14px;'>â€¢ {s}</div>",
                    unsafe_allow_html=True
                )
            st.markdown("<hr style='margin:6px 0; border:0.5px solid #eee;'>", unsafe_allow_html=True)

# ==========================================================
# ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (1ë‹¨ê³„ / 2ë‹¨ê³„ íƒ­ + ë‚ ì§œ íƒœê¹…)
# ==========================================================
today = datetime.date.today().strftime("%Y-%m-%d")

st.markdown("---")
st.subheader("ğŸ“‚ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

tab1, tab2 = st.tabs(["1ï¸. 1ë‹¨ê³„ ê²°ê³¼", "2ï¸. 2ë‹¨ê³„ ê²°ê³¼"])

# 1ï¸âƒ£ 1ë‹¨ê³„ ê²°ê³¼
with tab1:
    st.markdown("### 1ë‹¨ê³„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    st.caption("ëŒ€í‘œ_lv1 ê¸°ì¤€ìœ¼ë¡œ ë¬¶ì¸ ìƒí’ˆëª… ìë™ ë¶„ë¥˜ ê²°ê³¼ì…ë‹ˆë‹¤.")
    df_lv1_final = st.session_state.get("df_lv1")
    if df_lv1_final is not None and not df_lv1_final.empty:
        # (A) ë¶„ì„/ë¦¬í¬íŒ…ìš© ê°„ë‹¨ í˜•íƒœ
        engine_export_lv1 = df_lv1_final[["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "ëŒ€í‘œ_lv1ëª…", "pname"]].copy()
        st.download_button(
            label=f"ë¦¬í¬íŒ…ìš©(ê°„ë‹¨) CSV ë‹¤ìš´ë¡œë“œ ({today})",
            data=engine_export_lv1.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"auto_cluster_lv1_report_{today}.csv",
            mime="text/csv"
        )
        # (B) í˜„ì¬ ê¸°ì¤€ ì €ì¥ìš© (ì„ë² ë”©/ì¤‘ê°„ì»¬ëŸ¼ í¬í•¨)
        st.download_button(
            label=f" ê¸°ì¤€ ì €ì¥ìš©(ì¤‘ê°„ì»¬ëŸ¼ í¬í•¨) CSV ë‹¤ìš´ë¡œë“œ ({today})",
            data=df_lv1_final.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"auto_cluster_lv1_full_{today}.csv",
            mime="text/csv"
        )
    else:
        st.warning("ë¨¼ì € 1ë‹¨ê³„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

# 2ï¸âƒ£ 2ë‹¨ê³„ ê²°ê³¼
with tab2:
    st.markdown("### 2ë‹¨ê³„ ì„¸ë¶„í™” ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    st.caption("ëŒ€í‘œ_lv1 ë‚´ë¶€ì—ì„œ ì„¸ë¶„í™”ëœ ìƒí’ˆëª… ìë™ ë¶„ë¥˜ ê²°ê³¼ì…ë‹ˆë‹¤.")
    df_lv2_final = st.session_state.get("df_lv2")
    if df_lv2_final is not None and not df_lv2_final.empty:
        # (A) ë¦¬í¬íŒ…ìš© ê°„ë‹¨ í˜•íƒœ (cluster_lv2 í¬í•¨)
        engine_export_lv2 = df_lv2_final[["ì‚¬ì´íŠ¸ì´ë¦„", "L_category", "ëŒ€í‘œ_lv1ëª…", "cluster_lv2", "pname"]].copy()
        st.download_button(
            label=f"ë¦¬í¬íŒ…ìš©(ê°„ë‹¨) CSV ë‹¤ìš´ë¡œë“œ ({today})",
            data=engine_export_lv2.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"auto_cluster_lv2_report_{today}.csv",
            mime="text/csv"
        )
        # (B) í˜„ì¬ ê¸°ì¤€ ì €ì¥ìš© (ì„ë² ë”©/ì¤‘ê°„ì»¬ëŸ¼ í¬í•¨)
        st.download_button(
            label=f"ê¸°ì¤€ ì €ì¥ìš©(ì¤‘ê°„ì»¬ëŸ¼ í¬í•¨) CSV ë‹¤ìš´ë¡œë“œ ({today})",
            data=df_lv2_final.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"auto_cluster_lv2_full_{today}.csv",
            mime="text/csv"
        )
    else:
        st.warning("âš ï¸ ë¨¼ì € 2ë‹¨ê³„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
