{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af22c221",
   "metadata": {},
   "source": [
    "# Cluster_model (Blinded)\n",
    "\n",
    "이 노트북은 내부 고유명사/회사명 제거 및 중립화 처리된 연구용 버전입니다.\n",
    "실제 데이터/브랜드/서비스명은 포함하지 않습니다.\n",
    "\n",
    "코드 구현 상세는 `modules/clustering_core.py` 를 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fb720",
   "metadata": {},
   "source": [
    "1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load Data \n",
    "# -----------------------------\n",
    "# 실제 운영 환경에서는 웹 업로드 또는 DB 연동으로 대체될 수 있음\n",
    "data = pd.read_excel(\"sample_product_data.xlsx\")  \n",
    "\n",
    "# (Context) 이 단계는 상품명 분류 파이프라인 중,\n",
    "# '시리즈(Series) 단위로 묶어야 하는 대상만 선별'하는 1차 필터입니다.\n",
    "# 예: 특정 L_category에 상품이 충분히 많이 모여 있는 경우만 Series 후보로 간주\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Identify categories that need grouping\n",
    "# -----------------------------\n",
    "# 1) L_category별 데이터 수 집계\n",
    "count_df = data.groupby('L_category')['L_category'].count().reset_index(name='count')\n",
    "\n",
    "# 2) 시리즈로 묶을지 여부 결정 기준 (count > 3 이면 series 대상)\n",
    "#    - 기준값은 데이터 특성과 비즈니스 판단에 따라 유동적 (예: 3개 이상이면 패턴이 존재할 가능성 판단)\n",
    "count_df['series_yn'] = np.where(count_df['count'] > 3, 'y', 'n')\n",
    "\n",
    "# 3) 원본 데이터에 series 여부 병합\n",
    "use_df = data.merge(count_df[['L_category', 'series_yn']], on='L_category', how='left')\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 시리즈 대상만 필터링\n",
    "# -----------------------------\n",
    "df = use_df[use_df['series_yn'] == 'y']\n",
    "# df.to_excel(\"depth_test.xlsx\", index=False)  # (로컬 export 시 사용)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec3cba",
   "metadata": {},
   "source": [
    "2. 데이터 전처리를 통한 분류 패턴 유도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032520d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. 데이터 전처리 (Text Normalization & Noise Trimming)\n",
    "# -----------------------------\n",
    "# 이 단계는 \"좁혀 들어가는 방식\"의 전처리 전략을 사용합니다.\n",
    "# 특히 내부 콘텐츠 담당자들이 상품명을 작성할 때,\n",
    "# - 사이트명 / 대분류 / 세부 태그를 앞단 혹은 끝단에 붙이는 특성이 존재한다고 가정합니다.\n",
    "# 따라서 1차로 일반적인 특수문자/공백 정리 → 2차로 L_category / 사이트명 등을 제거해\n",
    "# \"실제 핵심 상품명만 남도록\" 트리밍합니다.\n",
    "#\n",
    "# ex) [브랜드패스] JLPT N5 끝장패키지 → 노이즈 제거 후 \"n5\"\n",
    "#     → 이후 클러스터링에 직접적으로 유용한 핵심 키워드만 남기는 전략\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"기본적인 특수문자/괄호 제거 전처리 + lower 정규화\"\"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[\\[\\]\\(\\)]\", \" \", text)  # 괄호 제거 (내부 한글은 유지)\n",
    "    text = re.sub(r\"[^가-힣A-Za-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# 특수 공백(Zero-width, NBSP 등) 정규화\n",
    "_WS_HARD = r\"[\\u00A0\\u202F\\u2007\\u2000-\\u2006\\u2008-\\u200A\\u200B-\\u200D\\u2060]\"\n",
    "def _normalize_spaces(s: str) -> str:\n",
    "    s = re.sub(_WS_HARD, \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# 문자열을 퍼지 매칭용 정규식으로 변환 (노이즈 제거용)\n",
    "def _build_fuzzy_pattern(cat: str) -> str:\n",
    "    base = re.sub(_WS_HARD, \"\", cat)\n",
    "    base = re.sub(r\"[\\s\\W_]+\", \"\", base)\n",
    "    parts = [re.escape(ch) for ch in base]\n",
    "    if not parts:\n",
    "        return r\"$^\"  # (빈 패턴 방지)\n",
    "    return r\"(?:[\\s\\W_]*)\".join(parts)\n",
    "\n",
    "def clean_and_trim_text(row):\n",
    "    \"\"\"\n",
    "    상품명(pname)에서 L_category / 사이트명 등을 fuzzy하게 제거하여\n",
    "    핵심 상품명만 남기는 'trim' 처리\n",
    "    \"\"\"\n",
    "    text = str(row[\"pname\"])\n",
    "    category = str(row[\"L_category\"])\n",
    "    site_name = str(row.get(\"site_name\", \"\"))  # 내부 필드명 노출 회피용 (브랜드나 채널명 등)\n",
    "\n",
    "    text = re.sub(r\"[\\[\\]\\(\\)]\", \" \", text)  # 괄호 제거\n",
    "    text_norm = _normalize_spaces(text)\n",
    "\n",
    "    # L_category 제거\n",
    "    if category and category.lower() != \"nan\":\n",
    "        cat_pat = _build_fuzzy_pattern(_normalize_spaces(category))\n",
    "        text_norm = re.sub(cat_pat, \" \", text_norm, flags=re.IGNORECASE)\n",
    "\n",
    "    # site_name 제거\n",
    "    if site_name and site_name.lower() != \"nan\":\n",
    "        site_pat = _build_fuzzy_pattern(_normalize_spaces(site_name))\n",
    "        text_norm = re.sub(site_pat, \" \", text_norm, flags=re.IGNORECASE)\n",
    "\n",
    "    # 다시 불필요한 특수문자 제거 + 공백/소문자 정규화\n",
    "    text_norm = re.sub(r\"[^가-힣A-Za-z0-9\\s]\", \" \", text_norm)\n",
    "    text_norm = _normalize_spaces(text_norm).lower()\n",
    "    return text_norm\n",
    "\n",
    "# 1차 기본 전처리\n",
    "df[\"product_name_clean\"] = df[\"pname\"].astype(str).apply(clean_text)\n",
    "# 2차 전처리 (카테고리 / 사이트명 제거 후 핵심만 남기기)\n",
    "df[\"product_name_trimmed\"] = df.apply(clean_and_trim_text, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 테스트용 필터 (프론트엔드에서 실제로 선택되는 조건 역할)\n",
    "# -----------------------------\n",
    "df_test = df[\n",
    "    (df[\"site_code\"] == \"sample_channel\") &\n",
    "    (df[\"L_category\"].isin([\"sample_L1\", \"sample_L2\", \"sample_L3\", \"sample_series_A\"]))\n",
    "].copy()\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"전처리 결과 샘플:\")\n",
    "df_test[[\"site_name\", \"L_category\", \"pname\", \"product_name_trimmed\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd66e1b",
   "metadata": {},
   "source": [
    "3. Bert 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d256094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Sentence-BERT 모델 로드\n",
    "# -----------------------------\n",
    "# 한국어 기반 상품명을 '의미 유사도' 기준으로 군집화하기 위해\n",
    "# HuggingFace 공개 모델을 사용합니다. (언어 의존적 유사도 학습에 최적화)\n",
    "# → 단순 문자열 매칭이 아닌 의미 embedding 기반 Clustering이 가능해짐\n",
    "\n",
    "model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")  \n",
    "# 위 모델은 한국어 문장 임베딩 성능 대비 속도가 빠른 편이라 실무형 분류에 적합하다고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b672c4b",
   "metadata": {},
   "source": [
    "4-1. 앞단 문맥 중심 임베딩에 집중한 1차 클러스터링 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. \"앞단 중심\" 임베딩 전략\n",
    "# -----------------------------\n",
    "# 교육/커머스 도메인에서는 상품명 앞단에 브랜드명 / 핵심 컨셉이 배치되는 경우가 매우 많음\n",
    "# 예)\n",
    "#   [브랜드명] JLPT N5 끝장패키지\n",
    "#   [플랫폼명] 토익 700+ 실전반\n",
    "# → 따라서 '문장 전체'가 아닌 '앞단 토큰'에 가중치를 더 주는 커스텀 Embedding 전략 사용\n",
    "\n",
    "def front_only_embedding(text, front_ratio=0.8, min_tokens=2, decay_rate=0.25):\n",
    "    tokens = re.split(r\"\\s+\", str(text).strip())\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "    # 앞단 토큰 비중 설정\n",
    "    split_idx = max(min_tokens, int(n * front_ratio))\n",
    "    front_tokens = tokens[:split_idx]\n",
    "    back_tokens = tokens[split_idx:]\n",
    "\n",
    "    # 앞단 토큰 고가중치 임베딩\n",
    "    front_emb = model.encode(front_tokens, show_progress_bar=False)\n",
    "    front_weights = np.array([np.exp(-decay_rate * i) for i in range(len(front_tokens))])\n",
    "    front_weights /= front_weights.sum()\n",
    "    front_vec = np.average(front_emb, axis=0, weights=front_weights)\n",
    "\n",
    "    # 뒷단도 너무 생략되지 않도록 0.01 비중으로 최소 반영\n",
    "    if back_tokens:\n",
    "        back_emb = model.encode(back_tokens, show_progress_bar=False)\n",
    "        back_vec = np.mean(back_emb, axis=0) * 0.01\n",
    "    else:\n",
    "        back_vec = np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "    return front_vec + back_vec\n",
    "\n",
    "# -----------------------------\n",
    "# 5. L_category별 1차 군집화 (semantic grouping)\n",
    "# -----------------------------\n",
    "# 동일 L_category 내에서 의미 기반으로 상품명 세분화\n",
    "# Agglomerative Clustering 활용 (사전 k 지정 없이 distance threshold 기반 자동 분기)\n",
    "# → 실제 실무에서는 사람이 수동으로 분류하던 패턴을 자동화하기 위한 시도\n",
    "\n",
    "cluster_results = []\n",
    "\n",
    "for cat in sorted(df_test[\"L_category\"].unique()):\n",
    "    subset = df_test[df_test[\"L_category\"] == cat].copy()\n",
    "    if len(subset) < 2:\n",
    "        print(f\"{cat} 카테고리는 상품이 {len(subset)}개라 스킵됨\")\n",
    "        continue\n",
    "\n",
    "    subset[\"embedding_front\"] = subset[\"product_name_trimmed\"].apply(\n",
    "        lambda x: front_only_embedding(x, front_ratio=0.7, min_tokens=2, decay_rate=0.25)\n",
    "    )\n",
    "    X_front = np.vstack(subset[\"embedding_front\"].values)\n",
    "\n",
    "    cluster_front = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=9,\n",
    "        metric='euclidean',\n",
    "        linkage='ward'\n",
    "    )\n",
    "    subset[\"cluster_lv1\"] = cluster_front.fit_predict(X_front)\n",
    "    cluster_results.append(subset)\n",
    "\n",
    "if not cluster_results:\n",
    "    raise ValueError(\"클러스터 결과가 비어 있습니다. df_test 조건 또는 데이터 확인 필요\")\n",
    "\n",
    "df_lv1 = pd.concat(cluster_results, ignore_index=True)\n",
    "print(f\"✅ 1차 클러스터링 완료: 총 {len(df_lv1)}개 상품 처리됨\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. 대표명(대표_lv1명) 생성\n",
    "# -----------------------------\n",
    "# 자동 분류된 클러스터에 사람이 '대표명'을 빠르게 부여할 수 있도록\n",
    "# 머신이 추천하는 초간단 Core Phrase 기반의 대표명 생성\n",
    "\n",
    "def extract_core_phrase(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\bv\\d+|\\bver\\d+|\\b\\d{2,4}(\\.\\d+)?\\b\", \"\", text)    # 버전 관련 제거\n",
    "    text = re.sub(r\"(교재|포함|미포함|only|ver|버전)\", \"\", text)      # 의미 분류에 불필요한 단어 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokens = text.split()\n",
    "    core = \" \".join(tokens[:3]) if len(tokens) > 3 else text\n",
    "    return core.strip()\n",
    "\n",
    "df_lv1[\"대표핵심어\"] = df_lv1[\"product_name_trimmed\"].apply(extract_core_phrase)\n",
    "\n",
    "rep_lv1 = (\n",
    "    df_lv1.groupby([\"site_name\", \"L_category\", \"cluster_lv1\"])\n",
    "    .agg(대표핵심어=(\"대표핵심어\", lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rep_lv1[\"대표_lv1명\"] = rep_lv1.apply(\n",
    "    lambda row: f\"{row['site_name']}_{row['L_category']}_{row['대표핵심어']}\", axis=1\n",
    ")\n",
    "\n",
    "df_lv1 = df_lv1.merge(\n",
    "    rep_lv1[[\"site_name\", \"L_category\", \"cluster_lv1\", \"대표_lv1명\"]],\n",
    "    on=[\"site_name\", \"L_category\", \"cluster_lv1\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. 결과 인사이트 프린트 (사람 검증용)\n",
    "# -----------------------------\n",
    "print(\"=== 1차 (L_category별 앞단 중심) Clustering 결과 ===\\n\")\n",
    "for cat, group in df_lv1.groupby(\"L_category\"):\n",
    "    print(f\"[{cat}] 카테고리 내 클러스터\")\n",
    "    for rep_name, subset in group.groupby(\"대표_lv1명\"):\n",
    "        print(f\" ▶ {rep_name} ({len(subset)}개)\")\n",
    "        for s in subset[\"product_name_trimmed\"].tolist():\n",
    "            print(f\"   - {s}\")\n",
    "        print()\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# 후속 단계에서 사용될 핵심 결과셋\n",
    "front_df_lv1 = (\n",
    "    df_lv1[['site_name', 'L_category', '대표_lv1명', 'pname']]\n",
    "    .sort_values(by=['L_category','대표_lv1명'])\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c594af",
   "metadata": {},
   "source": [
    "4-2. 뒷단 문맥 중심 임베딩에 집중한 2차 클러스터링 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e643d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 8. 1단계 이후 선택적으로 진행: \"뒷단 중심\" 세분화\n",
    "# ==========================================================\n",
    "# 목적:\n",
    "# - 1단계(앞단 중심)로 큰 묶음 생성\n",
    "# - 2단계는 버전/옵션/프로모션/기간 등 \"뒷단 신호\"로 세분화\n",
    "# - 계층: L_category → 대표_lv1명 → 대표_lv2명\n",
    "#\n",
    "# 결과물:\n",
    "# - 엑셀/사람 검증 친화형 컬럼 유지(site_name, L_category, 대표_lv1명, 대표_lv2명, pname)\n",
    "\n",
    "# -----------------------------\n",
    "# 8-1) \"뒷단 중심\" 임베딩 함수\n",
    "# -----------------------------\n",
    "def back_only_embedding(text, back_ratio=0.5, min_tokens=2, decay_rate=0.25):\n",
    "    \"\"\"\n",
    "    문장 뒤쪽 토큰에 가중치를 더 주는 임베딩.\n",
    "    - back_ratio: 뒤쪽으로 볼 비중(0~1)\n",
    "    - min_tokens: 최소 뒤쪽 토큰 수\n",
    "    - decay_rate: 뒤쪽 내부 가중치 감쇠율(뒤에서 앞으로 갈수록 감소)\n",
    "    \"\"\"\n",
    "    tokens = re.split(r\"\\s+\", str(text).strip())\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "    split_idx = max(n - max(min_tokens, int(n * back_ratio)), 0)\n",
    "    front_tokens = tokens[:split_idx]\n",
    "    back_tokens = tokens[split_idx:]\n",
    "\n",
    "    # 뒤쪽: 역순 가중 평균\n",
    "    back_emb = model.encode(back_tokens, show_progress_bar=False)\n",
    "    back_weights = np.array([np.exp(-decay_rate * i) for i in range(len(back_tokens))])[::-1]\n",
    "    back_weights /= back_weights.sum()\n",
    "    back_vec = np.average(back_emb, axis=0, weights=back_weights)\n",
    "\n",
    "    # 앞쪽: 거의 무시(0.01배)\n",
    "    if front_tokens:\n",
    "        front_emb = model.encode(front_tokens, show_progress_bar=False)\n",
    "        front_vec = np.mean(front_emb, axis=0) * 0.01\n",
    "    else:\n",
    "        front_vec = np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "    return back_vec + front_vec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8-2) 뒷단 중심 클러스터링\n",
    "# -----------------------------\n",
    "depth_threshold_lv2 = 5  # 낮추면 더 잘게, 올리면 덜 잘게\n",
    "cluster_results = []\n",
    "\n",
    "for lv1_name in sorted(df_lv1[\"대표_lv1명\"].unique()):\n",
    "    subset = df_lv1[df_lv1[\"대표_lv1명\"] == lv1_name].copy()\n",
    "\n",
    "    # 소규모 그룹은 세분화 생략\n",
    "    if len(subset) < 3:\n",
    "        subset[\"cluster_lv2\"] = 0\n",
    "        subset[\"대표_lv2명\"] = subset[\"대표_lv1명\"] + \"_세분화0\"\n",
    "        cluster_results.append(subset)\n",
    "        continue\n",
    "\n",
    "    # 임베딩(뒷단 중심)\n",
    "    subset[\"embedding_back\"] = subset[\"product_name_trimmed\"].apply(\n",
    "        lambda x: back_only_embedding(x, back_ratio=0.5, min_tokens=2, decay_rate=0.25)\n",
    "    )\n",
    "    X_back = np.vstack(subset[\"embedding_back\"].values)\n",
    "\n",
    "    # 거리 임계값 기반 계층 군집\n",
    "    cluster_back = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=depth_threshold_lv2,\n",
    "        metric=\"euclidean\",\n",
    "        linkage=\"ward\"\n",
    "    )\n",
    "    subset[\"cluster_lv2\"] = cluster_back.fit_predict(X_back)\n",
    "\n",
    "    # 대표_lv2명 생성(간단 규칙: first 사용; 필요시 mode 기반으로 변경 가능)\n",
    "    rep_lv2 = (\n",
    "        subset.groupby([\"대표_lv1명\", \"cluster_lv2\"])\n",
    "        .agg(대표_trim=(\"product_name_trimmed\", \"first\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    rep_lv2[\"대표_lv2명\"] = rep_lv2.apply(\n",
    "        lambda r: f\"{r['대표_lv1명']}_{r['대표_trim']}_세분화{r['cluster_lv2']}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    subset = subset.merge(\n",
    "        rep_lv2[[\"대표_lv1명\", \"cluster_lv2\", \"대표_lv2명\"]],\n",
    "        on=[\"대표_lv1명\", \"cluster_lv2\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    cluster_results.append(subset)\n",
    "\n",
    "# 통합\n",
    "if not cluster_results:\n",
    "    raise ValueError(\"2단계 클러스터 결과가 비어 있음. 상위 단계/필터 확인 필요.\")\n",
    "df_final = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8-3) 결과 확인(사람 검증용 콘솔 출력)\n",
    "# -----------------------------\n",
    "print(\"\\n=== 2단계 (뒷단 중심 세분화) 클러스터 결과 ===\\n\")\n",
    "for lv1_name, g1 in df_final.groupby(\"대표_lv1명\"):\n",
    "    print(f\"[1차 대표] {lv1_name}\")\n",
    "    for lv2_name, g2 in g1.groupby(\"대표_lv2명\"):\n",
    "        print(f\"  ▶ {lv2_name} ({len(g2)}개)\")\n",
    "        for s in g2[\"product_name_trimmed\"].tolist():\n",
    "            print(\"     -\", s)\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8-4) 엑셀/대시보드용 최종 DF\n",
    "# -----------------------------\n",
    "front_df_lv2 = (\n",
    "    df_final[[\"site_name\", \"L_category\", \"대표_lv1명\", \"대표_lv2명\", \"pname\"]]\n",
    "    .sort_values(by=[\"L_category\", \"대표_lv1명\", \"대표_lv2명\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 저장(선택)\n",
    "# front_df_lv2.to_excel(\"cluster_lv2_result.xlsx\", index=False)\n",
    "# df_final.to_excel(\"cluster_lv2_full_result.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usingbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
